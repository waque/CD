{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train missing values percentage: 99.015\n",
      "Test missing values percentage: 98.96875\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils_cd import (\n",
    "        split_dataset,\n",
    "        standard_deviation,\n",
    "        plot_comparison_results,\n",
    "        impute_values,\n",
    "        plot_results,\n",
    "        plot_param_improv,\n",
    "        plot_results_from_csv,\n",
    "        aps_classifier_statistics\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "base_clfs = [BernoulliNB(), DecisionTreeClassifier(), KNeighborsClassifier(), RandomForestClassifier(n_estimators=100)]\n",
    "#base_clfs = [RandomForestClassifier(n_estimators=100)]\n",
    "\n",
    "def print_missing_percentage(df, name='Dataset'):\n",
    "    print('{} missing values percentage: {}'.format(name, (df.shape[0] - df.dropna().shape[0]) / df.shape[0] * 100))\n",
    "\n",
    "CLASS = 'class'\n",
    "train = pd.read_csv('./aps_failure_training_set.csv',\n",
    "                        skiprows=20,keep_default_na=False, na_values='na')\n",
    "test = pd.read_csv('./aps_failure_test_set.csv',\n",
    "                        skiprows=20,keep_default_na=False, na_values='na')\n",
    "\n",
    "print_missing_percentage(train, 'Train')\n",
    "print_missing_percentage(test, 'Test')\n",
    "\n",
    "X_train, y_train = split_dataset(train, CLASS)\n",
    "X_test, y_test = split_dataset(test, CLASS)\n",
    "y_train = y_train.map({'pos': 1, 'neg': 0})\n",
    "y_test = y_test.map({'pos': 1, 'neg': 0})\n",
    "\n",
    "aps = pd.concat([X_train, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   level_0 level_1    0\n",
      "0   bb_000  bv_000  1.0\n",
      "1   ah_000  bg_000  1.0\n",
      "2   bv_000  cq_000  1.0\n",
      "3   bb_000  cq_000  1.0\n",
      "4   aa_000  bt_000  1.0\n",
      "5   bu_000  cq_000  1.0\n",
      "6   bu_000  bv_000  1.0\n",
      "7   bb_000  bu_000  1.0\n",
      "8   cf_000  co_000  1.0\n",
      "9   ad_000  cf_000  1.0\n",
      "10  ad_000  co_000  1.0\n"
     ]
    }
   ],
   "source": [
    "def attribute_corr(df, top):\n",
    "        \n",
    "    def get_redundant_pairs(df):\n",
    "        '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "        pairs_to_drop = set()\n",
    "        cols = df.columns\n",
    "        for i in range(0, df.shape[1]):\n",
    "            for j in range(0, i+1):\n",
    "                pairs_to_drop.add((cols[i], cols[j]))\n",
    "        return pairs_to_drop\n",
    "\n",
    "    def get_top_abs_correlations(df, n=5):\n",
    "        au_corr = df.corr().abs().unstack()\n",
    "        labels_to_drop = get_redundant_pairs(df)\n",
    "        au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "        return au_corr[0:n]\n",
    "\n",
    "    \n",
    "    return get_top_abs_correlations(df, top)\n",
    "\n",
    "correlated_features = attribute_corr(X_train, 11)\n",
    "correlated_features = correlated_features.reset_index()\n",
    "print(correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute sets {0: {'bb_000', 'bv_000', 'bu_000', 'cq_000'}, 1: {'ah_000', 'bg_000'}, 2: {'bt_000', 'aa_000'}, 3: {'cf_000', 'co_000', 'ad_000'}}\n",
      "Best attributes to select ['bb_000', 'bg_000', 'aa_000', 'cf_000']\n",
      "Excluded columns ['bv_000', 'bu_000', 'cq_000', 'ah_000', 'bt_000', 'co_000', 'ad_000']\n"
     ]
    }
   ],
   "source": [
    "equal_attrs = {}\n",
    "all_attrs = set([])\n",
    "sets = 0\n",
    "for index, corr in correlated_features.iterrows():\n",
    "    attr1 = corr['level_0']\n",
    "    attr2 = corr['level_1']\n",
    "    \n",
    "    present = False\n",
    "    if attr1 in equal_attrs:\n",
    "        equal_attrs[attr1].append(attr1)\n",
    "    for attr in equal_attrs:\n",
    "        if attr1 in equal_attrs[attr] or attr2 in equal_attrs[attr]:\n",
    "            equal_attrs[attr].add(attr1)\n",
    "            equal_attrs[attr].add(attr2)\n",
    "            present = True\n",
    "            \n",
    "    if not present:\n",
    "        equal_attrs[sets] = set([attr1, attr2])\n",
    "        sets += 1\n",
    "        \n",
    "best_attrs = []\n",
    "all_attrs = []\n",
    "for attrs in equal_attrs:\n",
    "    best_attr = None\n",
    "    best_nmr_missing = 600000\n",
    "    for attr in equal_attrs[attrs]:\n",
    "        all_attrs.append(attr)\n",
    "        nmr_missing = aps[attr].isna().sum()\n",
    "        if nmr_missing < best_nmr_missing:\n",
    "            best_attr = attr\n",
    "            best_nmr_missing = nmr_missing\n",
    "        \n",
    "    best_attrs.append(best_attr)\n",
    "\n",
    "excluded_columns = [item for item in all_attrs if item not in best_attrs]\n",
    "print('Attribute sets {}'.format(equal_attrs))\n",
    "print('Best attributes to select {}'.format(best_attrs))\n",
    "print('Excluded columns {}'.format(excluded_columns))\n",
    "\n",
    "aps = aps.drop(aps[excluded_columns], axis=1)\n",
    "X_train = X_train.drop(X_train[excluded_columns], axis=1)\n",
    "X_test = X_test.drop(X_test[excluded_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes with 0 standard deviation ['cd_000']\n"
     ]
    }
   ],
   "source": [
    "no_std_dev = []\n",
    "for col in aps:\n",
    "    std_dev = standard_deviation(aps[col])\n",
    "    if std_dev == 0:\n",
    "        no_std_dev.append(col)\n",
    "        \n",
    "print('Attributes with 0 standard deviation {}'.format(no_std_dev))\n",
    "aps = aps.drop(aps[no_std_dev], axis=1)\n",
    "X_train = X_train.drop(X_train[no_std_dev], axis=1)\n",
    "X_test = X_test.drop(X_test[no_std_dev], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_zero, X_test_zero = X_train.fillna(0), X_test.fillna(0)\n",
    "X_train_mean, X_test_mean = X_train.fillna(X_train.mean()), X_test.fillna(X_train.mean())\n",
    "X_train_median, X_test_median = X_train.fillna(X_train.median()), X_test.fillna(X_train.median())\n",
    "X_train_mfrequent, X_test_mfrequent = X_train, X_test\n",
    "for col in X_train:\n",
    "    mode = X_train[col].dropna().mode()[0]\n",
    "    X_train_mfrequent[col] = X_train_mfrequent[col].fillna(mode) \n",
    "    X_test_mfrequent[col] = X_test_mfrequent[col].fillna(mode) \n",
    "\n",
    "X_data = {'Zero replace': (X_train_zero, X_test_zero), 'Mean': (X_train_mean, X_test_mean), 'Median': (X_train_median, X_test_median), 'Most Frequent': (X_train_mfrequent, X_test_mfrequent)}\n",
    "#plot_comparison_results(base_clfs, X_data, y_train, y_test, technique='Technique', filename='missing_values', figsize=(20, 6))\n",
    "#plot_results_from_csv('missing_values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best results:\n",
    "- NB - Zero replace\n",
    "- Decision Tree - median replace\n",
    "- KNN - Mean\n",
    "- Random Forest - Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X_train, X_test):\n",
    "    normalizer = Normalizer().fit(X_train)\n",
    "\n",
    "    X_train_norm = normalizer.transform(X_train)\n",
    "    X_test_norm = normalizer.transform(X_test)\n",
    "    \n",
    "    return X_train_norm, X_test_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tentativa nos valores com bins de fazer uma nova feature. There are 7 attributes with bins from 0 to 9, let's try to make a new feature from this one using a weighted sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_transform(X_train, X_test):\n",
    "    bin_columns = ['ag_00', 'ay_00', 'az_00', 'ba_00', 'cn_00', 'cs_00', 'ee_00']\n",
    "\n",
    "    for column in bin_columns:\n",
    "        remove_cols = []\n",
    "        first_column = '{}0'.format(column)\n",
    "        remove_cols.append(first_column)\n",
    "        new_col = X_train[first_column]\n",
    "        new_col_test = X_test[first_column]\n",
    "        #print(X_train[first_column].mode()[0])\n",
    "        #print(X_train[first_column].value_counts()[0])\n",
    "\n",
    "        for i in range(1, 10):\n",
    "            col_name = '{}{}'.format(column, i)\n",
    "            #print(X_train[col_name].mode()[0])\n",
    "            #print(X_train[col_name].value_counts()[0])\n",
    "            new_col += X_train[col_name]*i\n",
    "            new_col_test += X_test[col_name]*i\n",
    "            \n",
    "        X_train = X_train.drop(columns=remove_cols)    \n",
    "        X_test = X_test.drop(columns=remove_cols)    \n",
    "        X_train = pd.concat([X_train, new_col], axis=1)\n",
    "        X_test = pd.concat([X_test, new_col_test], axis=1)\n",
    "\n",
    "        \n",
    "    return X_train, X_test\n",
    "        \n",
    "    \n",
    "X_train_nb, X_test_nb = bin_transform(X_train_zero, X_test_zero)\n",
    "X_train_dt, X_test_dt = bin_transform(X_train_mean, X_test_mean)\n",
    "X_train_knn, X_test_knn = bin_transform(X_train_median, X_test_median)\n",
    "X_train_rf, X_test_rf = bin_transform(X_train_mean, X_test_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predicted': array([0, 0, 0, ..., 0, 0, 0]), 'accuracy': 0.902125, 'confusion_matrix': array([[14083,  1542],\n",
      "       [   24,   351]]), 'sensibility': 0.936, 'specificity': 0.901312, 'score': 27420}\n"
     ]
    }
   ],
   "source": [
    "def normalize(X_train, X_test):\n",
    "    normalizer = Normalizer().fit(X_train)\n",
    "\n",
    "    X_train_norm = normalizer.transform(X_train)\n",
    "    X_test_norm = normalizer.transform(X_test)\n",
    "    \n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "clf = BernoulliNB()\n",
    "res = aps_classifier_statistics(clf, X_train_nb, X_test_nb, y_train, y_test)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select KBest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
